from openai import OpenAI
import streamlit as st
import os
from dotenv import load_dotenv
from services.prompt_builder import PromptBuild
from services.wv_search import WeaviateSearcher

load_dotenv()

os.environ["NO_PROXY"] = "localhost,127.0.0.1"

prompt_builder = PromptBuild()
searcher_wv = WeaviateSearcher()

st.title("S_MAKE")

client = OpenAI()

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

search_type = st.selectbox(
    "üîç –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –ø–æ–∏—Å–∫–∞:",
    ["–ü–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º", "–ü–æ –≤–µ–∫—Ç–æ—Ä—É (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π)", "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫"]
) 

print("–í–æ—Ç —Ç–∞–∫–æ–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –≤—ã–±—Ä–∞–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", search_type)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What is up?"):
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        print("–ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏...")
        chunks_dict = searcher_wv.search(prompt, 10, "Books", search_type)

        print("–°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç...")
        context_chunk = prompt_builder.build_prompt(chunks_dict, prompt)[1]
        context_prompt = prompt_builder.build_prompt(chunks_dict, prompt)[0]
        print("–í–æ—Ç –∫–æ–Ω–µ—á–Ω—ã–π –ø—Ä–æ–º–ø—Ç:", context_prompt)

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        st.session_state.messages.append({"role": "user", "content": prompt})

        print("–ü–µ—Ä–µ–¥–∞–µ–º –≤ –º–æ–¥–µ–ª—å...")
        stream = client.responses.create(
            model=st.session_state["openai_model"],
            input=[
                {"role": m["role"], "content": context_prompt}
                for m in st.session_state.messages
            ],
            stream=True,
        )

        def extract_text_from_stream(stream):
            for chunk in stream:
                if chunk.__class__.__name__ == "ResponseTextDeltaEvent":
                    yield chunk.delta

        st.markdown("**–û—Ç–≤–µ—Ç –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:**")
        response = st.write_stream(extract_text_from_stream(stream))
        with st.expander("**–û—Ç—Ä—ã–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ:**"):
            st.markdown(
            f"""
            <div style= "background-color:#1A1C23; padding: 25px; border-radius: 16px;">{context_chunk}</div>
            """,
            unsafe_allow_html=True
            )

